{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NFL Fantasy Football: Data Preprocessing\n",
    "\n",
    "The overall goal of this notebook is to import data collected from https://github.com/amcheste/fball_data_collection and perform data preprocessing. This includes removing data that is not needed for the scope of this project, setting null values to zero and creating a data frames that can be used for data analysis and feature engineering."
   ],
   "id": "78513e4aff315ab9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:11:31.370549Z",
     "start_time": "2024-11-09T18:11:31.366383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os"
   ],
   "id": "57086f9c1a9b8247",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download Data\n",
    "Due to the volume of data collected by https://github.com/amcheste/fball_data_collection, the output csv files are considered large files for git.  Therefore, various output csv files are stored in an [Object Storage](https://docs.oracle.com/en-us/iaas/Content/Object/Concepts/objectstorageoverview.htm) bucket in [Oracle Cloud Infrastructure (OCI)](https://docs.oracle.com/en-us/iaas/Content/GSG/Concepts/baremetalintro.htm).\n",
    "\n",
    "Since the collected data does not include any sensitive data and is publicly available on the internet I created a [Pre-Authenticated Request (PAR)](https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/usingpreauthenticatedrequests.htm), allowing non-authenticated users to download the files stored in the `DSC-412` bucket which is storing the various csv files containing data collected from ESPN's APIs.  This [PAR URL](https://objectstorage.us-ashburn-1.oraclecloud.com/p/gGzdsEKSIArLMAV1cP7SUkd6jSGF-P5wFn5kENCtQaABjvsLJkgJZ_vPi-27a8NL/n/id8zuxg6euyj/b/DSC-412/o/) will be valid for the duration of this class.\n",
    "\n",
    "The downloaded csv files will be stored in directories under the `tmp` directory in this project.  If the `tmp` directory does not exist it will be created.  It should also be noted that the `tmp` directory is included in the `.gitignore` to ensure these large files are not checked into the git repository.  \n",
    "\n",
    "The raw csv files will be stored under `tmp/00` with preprocessed data stored in `tmp/01` that will be used for data analysis and feature engineering in the `02_feature_engineering.ipnb` notebook."
   ],
   "id": "761c2c6b90048a63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:11:39.012998Z",
     "start_time": "2024-11-09T18:11:31.382773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Check to see if the tmp dirs exist in this project.  We will need them to store data between notebooks.\n",
    "if not os.path.isdir('../tmp'):\n",
    "    os.mkdir('../tmp')\n",
    "if not os.path.isdir('../tmp/00'):\n",
    "    os.mkdir('../tmp/00')\n",
    "if not os.path.isdir('../tmp/01'):\n",
    "    os.mkdir('../tmp/01')\n",
    "if not os.path.isdir('../tmp/02'):\n",
    "    os.mkdir('../tmp/02')\n",
    "\n",
    "#\n",
    "# Pre-Authenticated Request URL that can be used to download the collected data.\n",
    "BUCKET_URL = 'https://objectstorage.us-ashburn-1.oraclecloud.com/p/gGzdsEKSIArLMAV1cP7SUkd6jSGF-P5wFn5kENCtQaABjvsLJkgJZ_vPi-27a8NL/n/id8zuxg6euyj/b/DSC-412/o/'   \n",
    "\n",
    "#\n",
    "# Import positions raw data\n",
    "ret = requests.get(BUCKET_URL)\n",
    "if ret.status_code != 200:\n",
    "    print(\"ERROR: Failed to fetch data file paths\")\n",
    "    print(f\"ERROR: {ret.json()}\")\n",
    "    exit(255)\n",
    "    \n",
    "#\n",
    "# Loop through the list of objects in the bucket.  Each object is a csv file of collected data from ESPN and Yahoo.\n",
    "for object in ret.json()['objects']:\n",
    "    ret = requests.get(f'{BUCKET_URL}{object['name']}')\n",
    "    if ret.status_code != 200:\n",
    "        print(f\"ERROR: Failed to fetch {object['name']}\")\n",
    "        print(f\"ERROR: {ret.json()}\")\n",
    "        exit(255)\n",
    "    #\n",
    "    # Save a local copy of the CSV in the tmp/00 directory\n",
    "    file = open(f'../tmp/00/{object['name']}', \"w\")\n",
    "    file.write(ret.text)\n",
    "    file.close()"
   ],
   "id": "78d187a0326be04f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## NFL Positions\n",
    "\n",
    "Our first data file contains details regarding positions.  This includes the ESPN position ID that will be refrenced through the remainder of the data.  We can use this to help filter positions we care about.  For the scope of this project we will focus on the Quarterback position."
   ],
   "id": "8dca6028b24d9117"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T18:11:39.063328Z",
     "start_time": "2024-11-09T18:11:39.056368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Import positions raw data\n",
    "positions_df = pd.read_csv('../tmp/00/positions.csv')\n",
    "\n",
    "#\n",
    "# remove url column\n",
    "positions_df = positions_df.drop(columns=['url'])\n",
    "positions_df.set_index('id', inplace=True)\n",
    "\n",
    "#\n",
    "# Select the positions we care about and remove the remaining from our fantasy positions dataframe\n",
    "fantasy_positions = ['QB']\n",
    "fantasy_positions_df = positions_df[positions_df['abbreviation'].isin(fantasy_positions)]\n",
    "\n",
    "#\n",
    "# Print out the dataframe to validate and get the QB ID.\n",
    "fantasy_positions_df"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           name abbreviation\n",
       "id                          \n",
       "8   Quarterback           QB"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>abbreviation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quarterback</td>\n",
       "      <td>QB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## NFL Players\n",
    "Next we will want to import past and current stats for players.  This data was collected from ESPN and there are multiple categories of stats including general, passing, and rushing.  We will want to import all of these and create a single dataframe that includes all of these stats.\n"
   ],
   "id": "f2a1ad2dbd63b100"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:11:40.074648Z",
     "start_time": "2024-11-09T18:11:39.098091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Read in the top level player data\n",
    "players_df = pd.read_csv('../tmp/00/players.csv')\n",
    "\n",
    "#\n",
    "# Remove columns that are not needed\n",
    "players_df = players_df.drop(columns=['url', 'stats_log'])\n",
    "players_df.set_index('id', inplace=True)\n",
    "\n",
    "#\n",
    "# If a value is NaN set it to 0\n",
    "players_df.fillna(0, inplace=True)\n",
    "\n",
    "#\n",
    "# Read in general player stats\n",
    "player_stats_general = pd.read_csv('../tmp/00/player_general_stats.csv')\n",
    "#\n",
    "# Remove columns that are not needed.  We do not need a uuid for each stats and the net_total_yards and net_yards_per_game do not have any values set.\n",
    "player_stats_general = player_stats_general.drop(columns=['id', 'net_total_yards', 'net_yards_per_game'])\n",
    "\n",
    "#\n",
    "# If a value is NaN set it to 0\n",
    "player_stats_general.fillna(0, inplace=True)\n",
    "\n",
    "#\n",
    "# Perform an inner join based on player ID to merge these the player stats in a single dataframe.\n",
    "result = pd.merge(players_df, player_stats_general, left_on='id', right_on='player_id', how='inner')\n",
    "\n",
    "#\n",
    "# Read in the passing stats for each player.\n",
    "player_stats_passing = pd.read_csv('../tmp/00/player_passing_stats.csv')\n",
    "\n",
    "#\n",
    "# We do not need a uuid for each stat entry\n",
    "player_stats_passing = player_stats_passing.drop(columns=['id'])\n",
    "\n",
    "#\n",
    "# If a value is NaN set it to 0\n",
    "player_stats_passing.fillna(0, inplace=True)\n",
    "\n",
    "#\n",
    "# Merge the stats in a single dataframe based on player ID\n",
    "result = pd.merge(result, player_stats_passing, on='player_id', how='inner')\n",
    "\n",
    "#\n",
    "# Read in rushing stats for QBs that are mobile.  We will follow the same pattern as above.\n",
    "player_stats_rushing = pd.read_csv('../tmp/00/player_rushing_stats.csv')\n",
    "player_stats_rushing = player_stats_rushing.drop(columns=['id'])\n",
    "player_stats_rushing.fillna(0, inplace=True)\n",
    "\n",
    "result = pd.merge(result, player_stats_rushing, on='player_id', how='inner')\n",
    "\n",
    "\n",
    "#\n",
    "# Now that we have a single dataframe lets remove non-active players and players who are not QBs\n",
    "index_to_drop = result[result['active'] != True].index\n",
    "result = result.drop(index_to_drop)\n",
    "index_to_drop = result[result['status'] != 'Active'].index\n",
    "result = result.drop(index_to_drop)\n",
    "\n",
    "#\n",
    "# Remove non QB positions.  \n",
    "# TODO: Remove magic number and read this programmatically.\n",
    "index_to_drop = result[result['position'] != 8].index\n",
    "result = result.drop(index_to_drop)\n",
    "\n",
    "#\n",
    "# Save this dataframe for future use.\n",
    "result.to_csv('../tmp/01/player_stats.csv')\n",
    "\n"
   ],
   "id": "e81c35860f9d472",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Player Fantasy Points\n",
    "Next we will want to import the data set collected from Yahoo that contains per game stats from the 2024 season along with the calculated fantasy football points based on their default scoring scheme."
   ],
   "id": "c2883acb86c0bf20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:11:40.091849Z",
     "start_time": "2024-11-09T18:11:40.087334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Read in fantasy points\n",
    "fpts_df = pd.read_csv('../tmp/00/player_points.csv')\n",
    "\n",
    "#\n",
    "# Remove anyone who did not score any points since they most likely did not play\n",
    "index_to_drop = fpts_df[fpts_df['points'] == 0.00].index\n",
    "fpts_df = fpts_df.drop(index_to_drop)\n",
    "\n",
    "#\n",
    "# Save this for future use.\n",
    "fpts_df.to_csv('../tmp/01/player_points.csv', index=False)"
   ],
   "id": "19d4b188140e1428",
   "outputs": [],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
